# %% [markdown]
# # Problemas de C√°lculo y √Ålgebra

# %% [markdown]
# ## C√°lculo
# 
# El c√°lculo no es algo oscuro. Es el lenguaje para modelar comportamientos. El c√°lculo nos permite encontrar la tasa de cambios para optimizar una funci√≥n. Sin el c√°lculo, no podr√≠amos comprender completamente t√©cnicas como
# 
# Retropropagaci√≥n en redes neuronales
# 
# Regresi√≥n utilizando m√≠nimos cuadrados √≥ptimos
# 
# Maximizaci√≥n de la expectativa en el ajuste de modelos probabil√≠sticos

# %% [markdown]
# ### Ejercicio 1
# 
# Supongamos que, en mi oficina, me toma 10 segundos (tiempo) recorrer 25 metros (distancia) hasta la m√°quina de caf√©.
# Si queremos expresar la situaci√≥n anterior como una funci√≥n, ser√≠a:
# 
# distancia=velocidad√ótiempo
# 
# Por lo tanto, en este caso, la velocidad es la primera derivada de la funci√≥n de distancia mencionada. Dado que la velocidad describe la tasa de cambio de la distancia con respecto al tiempo, cuando las personas dicen que toman la primera derivada de una funci√≥n determinada, se refieren a encontrar la tasa de cambio de esa funci√≥n.
# 
# **Encuentra la velocidad y construye la funci√≥n lineal de la distancia $(d)$ con respecto al tiempo $(t)$, cuando $(t ‚àà [0,10])$.**

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Despejamos la velocidad sabiendo que d=v*t


def distanc(t):
    d = 2.5 * t
    return d
    
t = np.linspace(0,10,11)


# %%
# "Graficar la funci√≥n de distancia en el dominio (t)
plt.plot(t,distanc(t))
plt.title("Grafico distancia vs tiempo")
plt.xlabel('tiempo')
plt.ylabel("distancia")
plt.show()

# %%

diccionario = {'tiempo': t,
            'distancia': [distanc(t) for t in t]    
}
print(diccionario)

df = pd.DataFrame(diccionario)
print(df)

# %% [markdown]
# ### Ejercicio 2
# 
# Result√≥ que no caminaba a una velocidad constante hacia la m√°quina de caf√©, sino que estaba acelerando (mi velocidad aumentaba con el tiempo). Si mi velocidad inicial era 0, todav√≠a me tom√≥ 10 segundos viajar desde mi asiento hasta la m√°quina de caf√©, pero caminaba cada vez m√°s r√°pido.
# 
# $V_o$ = velocidad inicial = $0$
# 
# t = tiempo
# 
# a = aceleraci√≥n
# 
# **distancia** = $V_o * t + 0.5 * a * (t^2)$
# 
# **velocidad** = $V_o + a * t$
# 
# La primera derivada de la funci√≥n de velocidad es la aceleraci√≥n. Me doy cuenta de que la funci√≥n de velocidad est√° estrechamente relacionada con la funci√≥n de distancia.
# 
# **Encuentra el valor de la aceleraci√≥n y construye la funci√≥n cuadr√°tica para  $(t ‚àà [0,10])$. Adem√°s, crea un gr√°fico y una tabla.**

# %%
# Define y grafica la funci√≥n cuadr√°tica

# %%
# Crea un DataFrame

# %% [markdown]
# Antes del ejercicio 3, haremos una breve introducci√≥n al algoritmo de Descenso por Gradientes, el cual tendr√° una explicaci√≥n m√°s detallada en m√≥dulos futuros del bootcamp.
# 
# El algoritmo de Descenso por Gradientes es el h√©roe detr√°s de la familia de algoritmos de aprendizaje profundo. Cuando un algoritmo de esta familia se ejecuta, intenta minimizar el error entre la entrada de entrenamiento y la salida predicha. Esta minimizaci√≥n se realiza mediante algoritmos de optimizaci√≥n, y el descenso por gradientes es el m√°s popular.
# 
# Supongamos que tienes estos pares de entrada y salida:
# 
# ```py
# # Entrada:
# [
#  [1,2],
#  [3,4]
# ]
# 
# # Salida:
# [
#  [50],
#  [110]
# ]
# ```
# 
# Podemos estimar que si multiplicamos los valores de entrada por [10, 20], podemos obtener la salida como se muestra arriba.
# 
# ```py
# 1(10) + 2(20) = 50
# 
# 3(10) + 4(20) = 110
# ```
# 
# Cuando un algoritmo de aprendizaje autom√°tico comienza a ejecutarse, asigna valores aleatorios y hace una predicci√≥n.
# Supongamos que asign√≥ los valores [1,2]:
# 
# ```py
# 1(1) + 2(2) = 5
# 
# 3(1) + 4(2) = 11
# ```
# 
# Una vez que tiene las predicciones, calcula el error: la diferencia entre los datos reales y los datos predichos. Existen muchas formas de calcular el error, y se les llama funciones de p√©rdida.
# 
# Una vez que tenemos este valor, el algoritmo de optimizaci√≥n comienza a mostrar su funcionamiento, y establece nuevos valores que reemplazan a los valores aleatorios iniciales.
# 
# Y el ciclo contin√∫a hasta que se cumple una condici√≥n. Esa condici√≥n puede ser hacer el ciclo n veces, o hacerlo hasta que el error sea menor que un valor determinado.

# %% [markdown]
# Puede ser dif√≠cil entender descenso por gradientes sin comprender gradiente. As√≠ que, vamos a centrarnos en lo que es un gradiente. El gradiente muestra la direcci√≥n del mayor cambio de una funci√≥n escalar. El c√°lculo del gradiente se realiza con derivadas, as√≠ que empecemos con un ejemplo sencillo. Para calcular el gradiente, solo necesitamos recordar algunos c√°lculos de √°lgebra lineal de la escuela secundaria porque necesitamos calcular derivadas.
# 
# Supongamos que queremos encontrar el punto m√≠nimo de $f(x) = x^2$. La derivada de esa funci√≥n es $df(x)=2x$. 
# 
# El gradiente de $f(x)$ en el punto $x=-10$
# 
# es 
# 
# $df(-10)=-20$.
# 
# El gradiente de $f(x)$ en el punto $x=1$
# 
# es 
# 
# $df(1)=2$.
# 
# Ahora visualicemos $f(x)$ y esos puntos $x=-10$ y $x=1$ .

# %%
import numpy as np
import seaborn as sns

def f(x):
    return x**2

def df(x):
    return 2*x

def visualize(f, x=None):
    
    xArray = np.linspace(-10, 10, 100) 
    yArray = f(xArray)
    sns.lineplot(x=xArray, y=yArray)
    
    if x is not None:
        assert type(x) in [np.ndarray, list] # x deber√≠a ser un array de numpy o una lista
        if type(x) is list: # Si es una lista, convertir en un array de numpy
            x = np.array(x)

            
        y = f(x)
        sns.scatterplot(x=x, y=y, color='red')

# %%
visualize(f, x=[-10, 1])

# %% [markdown]
# El punto rojo en x=-10 no conoce la superficie sobre la que se encuentra, solo sabe las coordenadas del lugar donde est√° y su propio gradiente, que es -20. Y el otro punto rojo en x=1 no sabe la superficie en la que se encuentra; solo sabe las coordenadas de su posici√≥n y su gradiente, que es 2.
# 
# Con solo esta informaci√≥n, podemos decir que el punto rojo en x=-10 deber√≠a hacer un salto mayor que el de x=1 porque tiene un valor absoluto de gradiente mayor. El signo indica la direcci√≥n. El signo negativo (-) indica que el punto rojo en  x=-10 debe moverse hacia la derecha, mientras que el otro debe moverse hacia la izquierda.
# 
# En resumen, el punto rojo en x=-10 (gradient: -20)  deber√≠a hacer un salto m√°s grande hacia la derecha, y el punto rojo en x=1 (gradient: 2) deber√≠a hacer un salto m√°s peque√±o hacia la izquierda. 
# 
# Sabemos que la longitud del salto deber√≠a ser proporcional al gradiente, pero ¬øcu√°l es exactamente ese valor? No lo sabemos. As√≠ que, digamos que los puntos rojos deben moverse con una longitud de alpha * gradiente, donde alpha es solo un par√°metro.
# 
# Podemos decir que la nueva ubicaci√≥n del punto rojo debe calcularse con la siguiente f√≥rmula:
# 
# x = x - gradient * alpha

# %% [markdown]
# Ahora implementemos esto con **NumPy**. Comencemos visualizando la funci√≥n $f(x)=x^2$ y el punto $x=-10$.

# %%
visualize(f, x=[-10])

# %% [markdown]
# El siguiente c√≥digo implementa toda la l√≥gica explicada anteriormente:

# %%
def gradient_descent(x, nsteps=1):
    
    # collectXs es un array para almacenar c√≥mo cambi√≥ x en cada iteraci√≥n, para poder visualizarlo m√°s tarde
    
    collectXs = [x]
    
    # learning_rate es el valor que mencionamos como alpha en la secci√≥n anterior
    
    learning_rate = 1e-01
    
    for _ in range(nsteps):
        
        # La siguiente l√≠nea hace la verdadera magia
        # El siguiente valor de x se calcula restando el gradiente * learning_rate de s√≠ mismo
        # La intuici√≥n detr√°s de esta l√≠nea est√° en la secci√≥n anterior
        
        x -= df(x) * learning_rate 
        collectXs.append(x)
        
    # Retornamos una tupla que contiene
    # x -> el valor reciente de x despu√©s de nsteps 
    # collectXs -> todos los valores de x que se calcularon hasta ahora
    
    return x, collectXs


# %% [markdown]
# Antes de ejecutar un descenso por gradientes con 1000 pasos, ejecut√©moslo solo dos veces, un paso a la vez, para ver c√≥mo evoluciona x. 
# Comenzamos con x=-10, y evoluciona a x=-8.Sabemos que cuando x=0 ese es el **punto m√≠nimo**, as√≠ que s√≠, est√° evolucionando en la direcci√≥n correcta.

# %%
x=-10
x, collectedXs = gradient_descent(x, nsteps=1)
print(x)

# %%
# El siguiente paso comenzar√° en  at x=-8. Ejecutemos un descenso por gradientes durante 1 paso.

x, collectedXs = gradient_descent(x, nsteps=1)
print(x)

# %% [markdown]
# Llega a ùë•=‚àí6.4. Excelente. Ahora, ejecut√©moslo 1000 veces.

# %%
x, collectedXs = gradient_descent(x, nsteps=1000)
print(x)

# %%
visualize(f, x=collectedXs)

# %% [markdown]
# ### Ejercicio 3
# 
# Cuando llego a la m√°quina de caf√©, escucho a mi colega hablar sobre los costos unitarios de producir el 'producto B' para la empresa. A medida que la empresa produce m√°s unidades, los costos unitarios contin√∫an disminuyendo hasta un punto en el que comienzan a aumentar.
# 
# Para optimizar el costo de producci√≥n por unidad en su m√≠nimo y mejorar la eficiencia, la empresa necesitar√≠a encontrar el n√∫mero de unidades que deben producirse donde los costos unitarios de producci√≥n comienzan a cambiar de disminuir a aumentar.
# 
# **Construye la funci√≥n cuadr√°tica $f(x)=0.1(x)^2‚àí9x +4500$ en $x‚àà[0,100]$ para crear la funci√≥n de costo por unidad, y haz una conclusi√≥n.**

# %%
# Definir y graficar la funci√≥n

# %% [markdown]
# Vimos con el Descenso por Gradientes c√≥mo el punto rojo navega en un entorno que no conoce. Solo sabe las coordenadas de donde est√° y su gradiente. El punto rojo podr√≠a encontrar el punto m√≠nimo usando solo este conocimiento y el algoritmo de descenso por gradientes.
# 
# **Opcional**:
# 
# Implementa todos los pasos anteriores para crear un algoritmo de descenso por gradientes y ver c√≥mo evoluciona el costo por unidad, comenzando desde 0 unidades de producci√≥n.

# %% [markdown]
# ## √Ålgebra lineal

# %% [markdown]
# ### Ejercicio 1: Suma de dos matrices
# 
# Sup√≥n que tenemos dos matrices A y B.
# 
# ```py
# A = [[1,2],[3,4]]
# B = [[4,5],[6,7]]
# 
# luego tenemos
# A+B = [[5,7],[9,11]]
# A-B = [[-3,-3],[-3,-3]]
# ```
# 
# Suma ambas matrices usando Python con NumPy.

# %%
# import numpy as np

 
 
# Crear la primera matriz

 
# Crear la segunda matriz

 
# Imprimir elementos

 
# Sumar ambas matrices


# %% [markdown]
# ### Ejercicio 2: Suma de dos listas
# 
# Habr√° muchas situaciones en las que tendremos que encontrar una suma por √≠ndice de dos listas diferentes. Esto puede tener aplicaciones posibles en la programaci√≥n diaria. En este ejercicio, resolveremos el mismo problema de varias maneras en las que se puede realizar esta tarea.
# 
# Tenemos las siguientes dos listas:
# 
# ```py
# list1 = [2, 5, 4, 7, 3]
# list2 = [1, 4, 6, 9, 10]
# ```
# 
# Ahora, usemos c√≥digo en Python para demostrar la suma de dos listas.

# %%
# Naive method

# Inicializando listas
list1 = [2, 5, 4, 7, 3]
list2 = [1, 4, 6, 9, 10]
 
# Imprimir listas originales
print ("Original list 1 : " + str(list1))
print ("Original list 2 : " + str(list2))
 
# Usando m√©todo ingenuo para sumar dos listas
res_list = []
for i in range(0, len(list1)):
    res_list.append(list1[i] + list2[i])
 
# Imprimir lista resultante
print ("Resulting list is : " + str(res_list))

# %% [markdown]
# Ahora usa los siguientes tres m√©todos diferentes para realizar el mismo c√°lculo: suma de dos listas.

# %%
# Usar comprensi√≥n de listas para realizar la suma de las dos listas:


# Inicializando listas

 
# Imprimir listas originales

 
# Usando comprensi√≥n de listas para sumar dos listas

 
# Imprimir lista resultante


# %%
# Usar map() + add():


# Inicializando listas

 
# Imprimir listas originales

 
# Usando map() + add() para sumar dos listas

 
# Imprimir lista resultante

# %%
# Usar zip() + sum():


# Inicializando listas

 
# Imprimir listas originales

 
# Usando zip() + sum() para sumar dos listas

 
# Imprimir lista resultante

# %% [markdown]
# ### Ejercicio 3: Multiplicaci√≥n punto a punto
# 
# Tenemos dos matrices:
# 
# ```py
# matrix1 = [[1,7,3],
#  [4,5,2],
#  [3,6,1]]
# matrix2 = [[5,4,1],
#  [1,2,3],
#  [4,5,2]]
# ```
# 
# Una t√©cnica simple pero costosa para conjuntos de datos de entrada m√°s grandes es usar bucles for. En este ejercicio, primero utilizaremos bucles for anidados para iterar a trav√©s de cada fila y columna de las matrices, y luego realizaremos la misma multiplicaci√≥n usando NumPy.

# %%
# Usando un bucle for para ingresar dos matrices de tama√±o n x m
matrix1 = [[1,7,3],
 [4,5,2],
 [3,6,1]]
matrix2 = [[5,4,1],
 [1,2,3],
 [4,5,2]]
 
res = [[0 for x in range(3)] for y in range(3)]
 
# Explicit for loops
for i in range(len(matrix1)):
    for j in range(len(matrix2[0])):
        for k in range(len(matrix2)):
 
            # Matriz resultante
            res[i][j] += matrix1[i][k] * matrix2[k][j]
 
print(res)

# %%
# Importar bibliotecas

 
# Ingresar dos matrices

 
# Esto devolver√° el producto punto

 
# Imprimir la matriz resultante


# %% [markdown]
# Fuente: 
# 
# https://www.youtube.com/channel/UCXq-PLvYAX-EufF5RAPihVg
# 
# https://www.geeksforgeeks.org/
# 
# https://medium.com/@seehleung/basic-calculus-explained-for-machine-learning-c7f642e7ced3
# 
# https://blog.demir.io/understanding-gradient-descent-266fc3dcf02f


